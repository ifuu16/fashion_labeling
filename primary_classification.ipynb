{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10c8f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import io\n",
    "import itertools\n",
    "import sklearn.metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "# import datetime\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c0194fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset encoding explaination.\n",
    "#image labelling - 0=glasses/sunglasses. 1= trousers/jeans. 3= shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba424d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the datasets and preprocess\n",
    "\n",
    "data_train = np.load(r'Full Dataset/primary categories - Train.npz')\n",
    "data_validation = np.load(r'Full Dataset/primary categories - Validation.npz')\n",
    "data_test = np.load(r'Full Dataset/primary categories - Test.npz')\n",
    "\n",
    "\n",
    "#t extract the arrays from dataset into input(images) and target(labels)\n",
    "images_train = data_train['images']\n",
    "labels_train = data_train['labels']\n",
    "\n",
    "images_val = data_validation['images']\n",
    "labels_val = data_validation['labels']\n",
    "\n",
    "\n",
    "images_test = data_test['images']\n",
    "labels_test = data_test['labels']\n",
    "\n",
    "#Pixel-wise normalization of the training, validation and testing data\n",
    "images_train = images_train/255.0\n",
    "images_val  = images_val/255.0\n",
    "images_test = images_test/255.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab0b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the hyperparameters\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "\n",
    "#Define the hyperparamets to tune and the variations we want to test.\n",
    "HP_FILTER_SIZE = hp.HParam('filter_size', hp.Discrete([3]))\n",
    "HP_FILTER_NUM = hp.HParam('filter_num', hp.Discrete(['32','64']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "#Log the hyperparameter with the file writer\n",
    "with tf.summary.create_file_writer('Logs/Model 1/hparam_tuning/').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_FILTER_SIZE, HP_FILTER_NUM], \n",
    "        metrics= [hp.Metric(METRIC_ACCURACY, display_name='Accuracy')]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc26778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE THE MODEL AND TRAIN IT\n",
    "\n",
    "#Stop the model from overfitting ie whenever the validation loss increases\n",
    "#the code tells the model to stop when the val_loss starts to increase in two subsequent epochs\n",
    "\n",
    "#directory to store the cm logs  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "session_num = 1\n",
    "def train_model(hparams,session_num):\n",
    "    \n",
    "    \n",
    "#Create and train the model\n",
    "\n",
    "\n",
    "    \n",
    "    model = Sequential([\n",
    "                Conv2D(32, 3, activation= 'relu', input_shape=(120,90,3)),\n",
    "                MaxPooling2D(pool_size=(2,2)),\n",
    "                Conv2D(32, 3, activation= 'relu'),\n",
    "                MaxPooling2D(pool_size=(2,2)),\n",
    "                Flatten(),\n",
    "                Dense(3)\n",
    "    ])\n",
    "\n",
    "\n",
    "#describe the loss function\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "#Compile the model\n",
    "    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "    log_dir = f\"Logs/Model 1/fit/run-{session_num}\"\n",
    "\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    " #Create a Confusion Matrix\n",
    "        \n",
    "        \n",
    "        \n",
    "#Plot_to_image\n",
    "\n",
    "    def plot_to_cm_image(cm, class_labels):\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "       \n",
    "\n",
    "        sns.set(font_scale=1.2)  # Adjust the font size\n",
    "        sns.heatmap(cm, annot=True, fmt=\"0.2f\", cmap=\"Blues\", square=True,\n",
    "                    xticklabels=class_labels,\n",
    "                    yticklabels=class_labels, ax=ax)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.close(fig)\n",
    "               \n",
    "        \n",
    "# Convert the plot to a NumPy array\n",
    "    \n",
    "        fig.canvas.draw()\n",
    "        plt_image = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "#         plt_image = np.frombuffer(plt.gcf().canvas.tostring_rgb(), dtype=np.uint8)\n",
    "#         plt_image = plt_image.reshape(plt.gcf().canvas.get_width_height()[::-1] + (3,))\n",
    "        \n",
    "        #Convert the array image to a tensor image of 4 dimensions for compartibility with TensorFlow\n",
    "        image_tensor = tf.convert_to_tensor(plt_image)  # Convert to a TensorFlow tensor\n",
    "        image_tensor = tf.expand_dims(image_tensor, axis=0)  # Add the batch dimension\n",
    "\n",
    "        return image_tensor\n",
    "    \n",
    "       \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    # log the confusion matrix as a heatmap\n",
    "    \n",
    "    confusion_matrix_writer = tf.summary.create_file_writer(log_dir +'/cm')\n",
    "\n",
    "    def log_confusion_matrix_to_tensorboard(epoch,log=None):\n",
    "        val_predict_raw= model.predict(images_val)\n",
    "        val_predict = np.argmax(val_predict_raw, axis =1)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        #Get the confusion matrix\n",
    "        cm_raw = confusion_matrix(labels_val, val_predict).astype('float')\n",
    "        #Normalize the confusion matrix(cm) row-wise\n",
    "        cm = cm_raw/cm_raw.sum(axis=1)[:,np.newaxis]\n",
    "        cm = np.round(cm, 2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        class_labels= [\"Glasses/Sunglassses\", \"Trousers/Jeans\", \"Shoes\"]\n",
    "        cm_image = plot_to_cm_image(cm, class_labels)\n",
    "\n",
    "        #Log the cm as an iomage summary\n",
    "        with confusion_matrix_writer.as_default():\n",
    "\n",
    "            tf.summary.image('Confusion Matrix', cm_image, step=epoch)\n",
    "            \n",
    "\n",
    "\n",
    "    cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix_to_tensorboard)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor= 'val_loss',\n",
    "        mode = 'auto',\n",
    "        min_delta = 0,\n",
    "        patience = 2,\n",
    "        verbose = 0,\n",
    "        restore_best_weights = True\n",
    "    )\n",
    "\n",
    "\n",
    "#Train the model\n",
    "    model.fit(\n",
    "        images_train,\n",
    "        labels_train,\n",
    "        epochs = EPOCHS,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        callbacks = [tensorboard_callback, cm_callback, early_stopping],\n",
    "        validation_data = (images_val, labels_val),\n",
    "        verbose =2\n",
    "    )\n",
    "\n",
    "    _,accuracy = model.evaluate(images_val, labels_val)\n",
    "\n",
    "    model.save(f\"Saved/Model 1/run-{session_num}\")\n",
    "\n",
    "\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fb505ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Starting trial: run-1\n",
      "{'filter_size': 3, 'filter_num': '32'}\n",
      "51/51 [==============================] - 2s 29ms/step\n",
      "203/203 - 41s - loss: 0.0864 - accuracy: 0.9722 - val_loss: 0.0062 - val_accuracy: 0.9994 - 41s/epoch - 201ms/step\n",
      "51/51 [==============================] - 2s 30ms/step - loss: 0.0062 - accuracy: 0.9994\n",
      "INFO:tensorflow:Assets written to: Saved/Model 1/run-1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Saved/Model 1/run-1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Starting trial: run-2\n",
      "{'filter_size': 3, 'filter_num': '64'}\n",
      "51/51 [==============================] - 2s 29ms/step\n",
      "203/203 - 41s - loss: 0.0967 - accuracy: 0.9674 - val_loss: 0.0048 - val_accuracy: 0.9988 - 41s/epoch - 203ms/step\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.0048 - accuracy: 0.9988\n",
      "INFO:tensorflow:Assets written to: Saved/Model 1/run-2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Saved/Model 1/run-2\\assets\n"
     ]
    }
   ],
   "source": [
    "#Log the hyperparameter to the TensorBoard\n",
    "\n",
    "\n",
    "def run(log_dir,hparams,session_num):\n",
    "    hyperparameter_writer = tf.summary.create_file_writer(log_dir)\n",
    "    \n",
    "    \n",
    "    with hyperparameter_writer.as_default():\n",
    "        hp.hparams(hparams) #record the values used in this trial\n",
    "        accuracy = train_model(hparams, session_num)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#Train the model with the different hyperparameters\n",
    "session_num:1\n",
    "for filter_size in HP_FILTER_SIZE.domain.values:\n",
    "    for filter_num in HP_FILTER_NUM.domain.values:\n",
    "        hparams ={\n",
    "        HP_FILTER_SIZE: filter_size,\n",
    "        HP_FILTER_NUM : filter_num\n",
    "        }\n",
    "        \n",
    "        run_name = \"run-%d\" % session_num\n",
    "        print('---Starting trial: %s' % run_name)\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "        run('Logs/Model 1/hparam_tuning/' + run_name, hparams, session_num)\n",
    "        \n",
    "        session_num += 1\n",
    "       \n",
    "        \n",
    "\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a19ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a38f19ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-388c8002984d3a9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-388c8002984d3a9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir 'Logs/Model 1/hparam_tuning'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e1505de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6db4a443b047cca4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6db4a443b047cca4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir 'Logs/Model 1/fit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29b717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
